import os
from dotenv import load_dotenv

# 1. åŠ è½½ .env ä¸­çš„ API Key
load_dotenv()

# æ£€æŸ¥ Key æ˜¯å¦åŠ è½½æˆåŠŸ
if not os.getenv("GOOGLE_API_KEY") or not os.getenv("OPENAI_API_KEY"):
    print("âŒ é”™è¯¯ï¼šè¯·æ£€æŸ¥ .env æ–‡ä»¶ï¼Œç¡®ä¿ GOOGLE_API_KEY å’Œ OPENAI_API_KEY å·²æ­£ç¡®å¡«å†™ï¼")
    exit()

# --- å¯¼å…¥ LangChain ç»„ä»¶ ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 2. é…ç½®æ¨¡å‹ (æ··åˆæ¶æ„)

# Chat æ¨¡å‹ï¼šä½¿ç”¨ Google Gemini 2.0 Flash
# å¦‚æœ 2.0 é¢„è§ˆç‰ˆä¸ç¨³å®šï¼Œå¯ä»¥éšæ—¶æ”¹å› "gemini-1.5-flash"
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp", 
    temperature=0
)

# Embedding æ¨¡å‹ï¼šä½¿ç”¨ OpenAI text-embedding-3-small
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# 3. åŠ è½½å¹¶å¤„ç†æ•°æ® (æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†)
print("ğŸ“‚ æ­£åœ¨æ‰«æ rag_docs æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶ (å«å­ç›®å½•)...")

try:
    # DirectoryLoader é…ç½®è¯´æ˜ï¼š
    # path: ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„
    # glob: "**/*.md" è¡¨ç¤ºé€’å½’æŸ¥æ‰¾æ‰€æœ‰å­æ–‡ä»¶å¤¹é‡Œçš„ markdown æ–‡ä»¶
    # loader_cls: å¼ºåˆ¶ä½¿ç”¨ TextLoader (çº¯æ–‡æœ¬æ¨¡å¼)ï¼Œé¿å…å®‰è£…å¤æ‚çš„ unstructured åº“
    # loader_kwargs: å¿…é¡»æŒ‡å®š utf-8ï¼Œå¦åˆ™è¯»å–ä¸­æ–‡/æ—¥æ–‡æ–‡ä»¶ä¼šæŠ¥é”™
    loader = DirectoryLoader(
        path="./rag_docs", 
        glob="**/*.md", 
        loader_cls=TextLoader,
        loader_kwargs={'encoding': 'utf-8'}
    )
    
    docs = loader.load()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡ä»¶ã€‚")

    # æ–‡æœ¬åˆ‡åˆ† (Chunking)
    # chunk_size=1000: æ¯ä¸ªç‰‡æ®µçº¦ 1000 å­—ç¬¦
    # chunk_overlap=200: ç‰‡æ®µä¹‹é—´é‡å  200 å­—ç¬¦ï¼Œä¿è¯ä¸Šä¸‹æ–‡è¿è´¯
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    print(f"âœ‚ï¸  åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(splits)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")
    print("ğŸš€ æ­£åœ¨å»ºç«‹å‘é‡æ•°æ®åº“ (è°ƒç”¨ OpenAI Embedding API)...")

    # 4. å»ºç«‹å‘é‡æ•°æ®åº“
    vectorstore = FAISS.from_documents(splits, embeddings)
    retriever = vectorstore.as_retriever()
    print("ğŸ’¾ å‘é‡æ•°æ®åº“å»ºç«‹å®Œæ¯•ï¼")

except Exception as e:
    print(f"âŒ è¯»å–æ–‡ä»¶å‡ºé”™: {e}")
    print("è¯·æ£€æŸ¥ï¼š1. rag_docs æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ 2. æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ markdown æ ¼å¼")
    exit()

# 5. å®šä¹‰ RAG çš„ Prompt æ¨¡æ¿
template = """
ä½ æ˜¯ä¸€ä¸ªç²¾é€š MBTI äººæ ¼ç†è®ºçš„ä¸“å®¶åŠ©æ‰‹ã€‚
è¯·åŸºäºä¸‹é¢çš„ã€èƒŒæ™¯ä¿¡æ¯ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
å¦‚æœèƒŒæ™¯ä¿¡æ¯é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ã€‚

ã€èƒŒæ™¯ä¿¡æ¯ã€‘ï¼š
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{question}
"""
prompt = ChatPromptTemplate.from_template(template)

# 6. æ„å»º RAG é“¾
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# --- äº¤äº’å¼é—®ç­”å¾ªç¯ ---
print("\n=== ğŸ¤– MBTI æ™ºèƒ½åŠ©æ‰‹å·²å°±ç»ª (è¾“å…¥ 'exit' é€€å‡º) ===")

while True:
    user_input = input("\nè¯·æé—® (ä¾‹å¦‚: ENFJçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆ?): ")
    if user_input.lower() in ["exit", "quit", "q"]:
        print("å†è§ï¼ğŸ‘‹")
        break
    
    if not user_input.strip():
        continue

    print("Thinking...", end="", flush=True)
    try:
        response = rag_chain.invoke(user_input)
        # æ¸…é™¤ "Thinking..." å¹¶æ‰“å°å›ç­”
        print(f"\r{' ' * 20}\r", end="") 
        print(f"ğŸ—£ï¸  å›ç­”: {response}")
    except Exception as e:
        print(f"\nâŒ è°ƒç”¨å‡ºé”™: {e}")