import os
from dotenv import load_dotenv

# 1. åŠ è½½ .env ä¸­çš„ API Key
load_dotenv(os.path.join(os.path.dirname(__file__), 'env', '.env'))

# æ£€æŸ¥ Key æ˜¯å¦åŠ è½½æˆåŠŸ
if not os.getenv("GOOGLE_API_KEY") or not os.getenv("OPENAI_API_KEY"):
    print("âŒ é”™è¯¯ï¼šè¯·æ£€æŸ¥ .env æ–‡ä»¶ï¼Œç¡®ä¿ GOOGLE_API_KEY å’Œ OPENAI_API_KEY å·²æ­£ç¡®å¡«å†™ï¼")
    exit()

# --- å¯¼å…¥ LangChain ç»„ä»¶ ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 2. é…ç½®æ¨¡å‹ (æ··åˆæ¶æ„)

# Chat æ¨¡å‹ï¼šä½¿ç”¨ Google Gemini 2.0 Flash
# å¦‚æœ 2.0 é¢„è§ˆç‰ˆä¸ç¨³å®šï¼Œå¯ä»¥éšæ—¶æ”¹å› "gemini-1.5-flash"
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp", 
    temperature=0
)

# Embedding æ¨¡å‹ï¼šä½¿ç”¨ OpenAI text-embedding-3-small
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# 3. åŠ è½½å¹¶å¤„ç†æ•°æ® - é’ˆå¯¹ä¸‰ä¸ªè¯­è¨€ç‰ˆæœ¬
def load_and_build_vectorstore(language, folder_path):
    """
    ä¸ºæŒ‡å®šè¯­è¨€åŠ è½½æ–‡æ¡£å¹¶å»ºç«‹å‘é‡æ•°æ®åº“
    """
    print(f"\nğŸ“‚ æ­£åœ¨æ‰«æ {folder_path} æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶...")
    
    try:
        loader = DirectoryLoader(
            path=folder_path, 
            glob="*.md", 
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        
        docs = loader.load()
        print(f"âœ… {language} ç‰ˆæœ¬ï¼šæˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡ä»¶ã€‚")

        # æ–‡æœ¬åˆ‡åˆ† (Chunking)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        
        print(f"âœ‚ï¸  åˆ‡åˆ†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(splits)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")
        print(f"ğŸš€ æ­£åœ¨ä¸º {language} ç‰ˆæœ¬å»ºç«‹å‘é‡æ•°æ®åº“...")

        # å»ºç«‹å‘é‡æ•°æ®åº“
        vectorstore = FAISS.from_documents(splits, embeddings)
        vectorstore.save_local(f"./rag_vectorstore_{language}")
        print(f"ğŸ’¾ {language} ç‰ˆæœ¬çš„å‘é‡æ•°æ®åº“å·²ä¿å­˜ï¼")
        
        return vectorstore
        
    except Exception as e:
        print(f"âŒ {language} ç‰ˆæœ¬è¯»å–æ–‡ä»¶å‡ºé”™: {e}")
        return None

# åŠ è½½ä¸‰ä¸ªè¯­è¨€ç‰ˆæœ¬çš„æ•°æ®
languages = {
    "cn": "./rag_docs/cn",
    "en": "./rag_docs/en",
    "jp": "./rag_docs/jp"
}

vectorstores = {}
for lang_code, folder_path in languages.items():
    if os.path.exists(folder_path):
        vectorstores[lang_code] = load_and_build_vectorstore(lang_code, folder_path)
    else:
        print(f"âš ï¸  è­¦å‘Šï¼š{folder_path} æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡ {lang_code} ç‰ˆæœ¬")

if not vectorstores:
    print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•è¯­è¨€ç‰ˆæœ¬çš„æ•°æ®!")
    exit()

print(f"\nâœ… æˆåŠŸåŠ è½½ {len(vectorstores)} ä¸ªè¯­è¨€ç‰ˆæœ¬çš„æ•°æ®åº“")

# 5. å®šä¹‰ RAG çš„ Prompt æ¨¡æ¿
templates = {
    "cn": """
ä½ æ˜¯ä¸€ä¸ªç²¾é€š MBTI äººæ ¼ç†è®ºçš„ä¸“å®¶åŠ©æ‰‹ã€‚
è¯·åŸºäºä¸‹é¢çš„ã€èƒŒæ™¯ä¿¡æ¯ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
å¦‚æœèƒŒæ™¯ä¿¡æ¯é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ã€‚

ã€èƒŒæ™¯ä¿¡æ¯ã€‘ï¼š
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{question}
""",
    "en": """
You are an expert assistant proficient in MBTI personality theory.
Please answer the user'sã€questionã€‘based on the followingã€background informationã€‘.
If the background information does not contain the answer, please honestly say you don't know, don't make it up.

ã€Background Informationã€‘:
{context}

ã€User Questionã€‘:
{question}
""",
    "jp": """
ã‚ãªãŸã¯MBTIäººæ ¼ç†è«–ã«ç²¾é€šã—ãŸå°‚é–€å®¶ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã€èƒŒæ™¯æƒ…å ±ã€‘ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€è³ªå•ã€‘ã«ç­”ãˆã¦ãã ã•ã„ã€‚
èƒŒæ™¯æƒ…å ±ã«ç­”ãˆãŒãªã„å ´åˆã¯ã€æ­£ç›´ã«çŸ¥ã‚‰ãªã„ã¨è¨€ã£ã¦ãã ã•ã„ã€‚ä½œã‚Šè©±ã‚’ã—ãªã„ã§ãã ã•ã„ã€‚

ã€èƒŒæ™¯æƒ…å ±ã€‘ï¼š
{context}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘ï¼š
{question}
"""
}

prompts = {lang: ChatPromptTemplate.from_template(template) for lang, template in templates.items()}

# 6. æ„å»ºä¸‰ä¸ªè¯­è¨€ç‰ˆæœ¬çš„ RAG é“¾
rag_chains = {}
for lang_code, vectorstore in vectorstores.items():
    retriever = vectorstore.as_retriever()
    rag_chains[lang_code] = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompts[lang_code]
        | llm
        | StrOutputParser()
    )
    print(f"âœ… {lang_code.upper()} RAG é“¾å»ºç«‹å®Œæˆ")

# --- äº¤äº’å¼é—®ç­”å¾ªç¯ ---
if __name__ == "__main__":
    print("\n=== ğŸ¤– MBTI æ™ºèƒ½åŠ©æ‰‹å·²å°±ç»ª ===")
    print("æ”¯æŒè¯­è¨€: CN (ä¸­æ–‡), EN (è‹±æ–‡), JP (æ—¥æ–‡)")
    print("è¾“å…¥ 'exit' é€€å‡º\n")

    current_language = "cn"  # é»˜è®¤ä¸­æ–‡

    while True:
        lang_hint = f"[{current_language.upper()}]"
        user_input = input(f"\n{lang_hint} è¯·æé—® (æˆ–è¾“å…¥ 'lang' åˆ‡æ¢è¯­è¨€): ")
        
        # å¤„ç†è¯­è¨€åˆ‡æ¢
        if user_input.lower() == "lang":
            print("\né€‰æ‹©è¯­è¨€: CN (ä¸­æ–‡) | EN (è‹±æ–‡) | JP (æ—¥æ–‡)")
            lang_choice = input("è¾“å…¥è¯­è¨€ä»£ç : ").lower()
            if lang_choice in rag_chains:
                current_language = lang_choice
                print(f"âœ… å·²åˆ‡æ¢è‡³ {lang_choice.upper()} ç‰ˆæœ¬")
            else:
                print(f"âŒ ä¸æ”¯æŒçš„è¯­è¨€ä»£ç : {lang_choice}")
            continue
        
        if user_input.lower() in ["exit", "quit", "q"]:
            print("å†è§ï¼ğŸ‘‹")
            break
        
        if not user_input.strip():
            continue

        print("Thinking...", end="", flush=True)
        try:
            response = rag_chains[current_language].invoke(user_input)
            # æ¸…é™¤ "Thinking..." å¹¶æ‰“å°å›ç­”
            print(f"\r{' ' * 20}\r", end="") 
            print(f"ğŸ—£ï¸  å›ç­”: {response}")
        except Exception as e:
            print(f"\nâŒ è°ƒç”¨å‡ºé”™: {e}")

# --- æ–°å¢ï¼šç”¨äº BlueSky åˆ†æçš„å‡½æ•° ---
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class PersonalityAnalysis(BaseModel):
    mbti: str = Field(description="æ¨æ–­çš„ MBTI ç±»å‹ï¼Œä¾‹å¦‚ INTJ")
    animal: str = Field(description="æ¨æ–­çš„åŠ¨ç‰©å åœå½¢è±¡ï¼Œä¾‹å¦‚ é»‘è±¹")
    description: str = Field(description="ç®€çŸ­çš„æ€§æ ¼ç”»åƒæè¿°ï¼Œçº¦ 50-100 å­—")

def analyze_personality(text_content):
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬å†…å®¹ï¼Œåˆ†æ MBTI å’ŒåŠ¨ç‰©å½¢è±¡ã€‚
    è¿”å› JSON æ ¼å¼æ•°æ®ã€‚
    """
    parser = JsonOutputParser(pydantic_object=PersonalityAnalysis)
    
    prompt = ChatPromptTemplate.from_template(
        """
        ä½ æ˜¯ä¸€ä¸ªç²¾é€š MBTI äººæ ¼ç†è®ºå’ŒåŠ¨ç‰©å åœçš„å¿ƒç†åˆ†æä¸“å®¶ã€‚
        è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç”¨æˆ·çš„ç¤¾äº¤åª’ä½“å†…å®¹ï¼ˆåŒ…æ‹¬ç®€ä»‹å’Œå¸–å­ï¼‰ï¼Œæ·±å…¥åˆ†æå…¶è¨€è¡Œé£æ ¼ã€ä»·å€¼è§‚å’Œæ€ç»´æ¨¡å¼ã€‚

        ã€ç”¨æˆ·å†…å®¹ã€‘ï¼š
        {text}

        è¯·æ¨æ–­ï¼š
        1. è¯¥ç”¨æˆ·çš„ MBTI ç±»å‹ (16å‹äººæ ¼)ã€‚
        2. è¯¥ç”¨æˆ·åœ¨â€œåŠ¨ç‰©å åœâ€ä¸­å¯¹åº”çš„åŠ¨ç‰©å½¢è±¡ (Animal Fortune)ã€‚
        3. ç”Ÿæˆä¸€æ®µç®€çŸ­çš„æ€§æ ¼ç”»åƒã€‚

        è¯·åŠ¡å¿…æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å« Markdown æ ¼å¼æ ‡è®° (```json ... ```)ã€‚
        
        {format_instructions}
        """
    )

    chain = prompt | llm | parser

    try:
        print("ğŸ§  æ­£åœ¨è¿›è¡Œ AI äººæ ¼åˆ†æ...")
        result = chain.invoke({
            "text": text_content,
            "format_instructions": parser.get_format_instructions()
        })
        return result
    except Exception as e:
        print(f"âŒ AI åˆ†æå¤±è´¥: {e}")
        return {
            "mbti": "Unknown",
            "animal": "Unknown", 
            "description": "åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
        }